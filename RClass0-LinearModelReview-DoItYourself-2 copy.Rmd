---
title: "RClass1-LinearModelReview"
author: "Caiwei Xiong"
date: "07/01/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Review of some aspects of the Linear Model

# Continuous vs. factor predictors

Load the libraries \texttt{foreign} (to be able to read in the data file) and \texttt{arm}, and read in the data \texttt{kidiq}. This is how it works on my computer: 

```{r message=FALSE}
library(foreign)
library(arm)

kidiq <- read.dta("kidiq.dta")
attach(kidiq)
head(kidiq)
```
The response is \texttt{kid\_score}, the child's score in an IQ test, \texttt{mom\_hs} (whether or not the mom has a highschool degree), \texttt{mom\_iq} (mom's IQ), \texttt{mom\_work} (mom's working pattern in the first years of the child's life, as described in Lecture 2a), and \texttt{mom\_age} (mom's age).

Next, fit two linear models to these data: 

\texttt{fit1}, which has \texttt{mom\_iq} and \texttt{mom\_work} as main effects and no interaction. In this model,  \texttt{mom\_work} is treated as a factor predictor.

```{r}
fit1 <- lm(kid_score~mom_iq+as.factor(mom_work))
summary(fit1)
```

Explain how many parameters this model has and why.
Answer:
There are five parameters in this model. Because \texttt{mom\_work} is treated as a factor predictor, which need to seperate the \texttt{mom\_work} into three different predictors. Those three predictors will demonstrate three different mon'work situations. 

\texttt{fit2}, which has \texttt{mom\_iq} and \texttt{mom\_work} as main effects and no interaction. However,  \texttt{mom\_work} is treated as a continuous predictor.

```{r}
fit2 <- lm(kid_score~mom_iq+mom_work)
summary(fit2)
```

Explain how many parameters this model has and why.
Answer:
Only three parameters in this model.(i.e. \texttt{mom\_iq}, \texttt{mom\_work}and intercept). Because the model set the \texttt{mom\_iq}, \texttt{mom\_work} as predictors and intercept will appear in every model. 

For \texttt{fit1} and \texttt{fit2}, print the first $5$ rows of the design matrix. Comment on how they differ.
Answer:
The design matrix for the \texttt{fit1} only has three columns. The design matrix for the \texttt{fit2} has five columns.
```{r}
head(model.matrix(fit1))
head(model.matrix(fit2))
```

Plot the data along with the fitted regression curve(s). For example, for \texttt{fit2}, I did it as follows (don't forget to remove the chunk option \texttt{eval=FALSE} to run the code): 

```{r}
plot(mom_iq,kid_score, xlab="Mother IQ score", 
  ylab="Child test score",pch=20, xaxt="n", yaxt="n", type="n")
curve (coef(fit2)[1] + 2*coef(fit2)[2] + (coef(fit2)[3])*x, add=TRUE, col="magenta")
curve (coef(fit2)[1] + coef(fit2)[2] + (coef(fit2)[3])*x, col="red", add=TRUE)
curve (coef(fit2)[1] + 3*coef(fit2)[2] + (coef(fit2)[3])*x, add=TRUE, col="blue")
curve (coef(fit2)[1] + 4*coef(fit2)[2] + (coef(fit2)[3])*x, add=TRUE, col="black")
points (mom_iq[mom_work==1], kid_score[mom_work==1], pch=20,col="red")
points (mom_iq[mom_work==2], kid_score[mom_work==2], pch=20,col="magenta")
points (mom_iq[mom_work==3], kid_score[mom_work==3], pch=20,col="blue")
points (mom_iq[mom_work==4], kid_score[mom_work==4], pch=20,col="black")
axis (1, c(80,100,120,140))
axis (2, c(20,60,100,140))
```

Create an analogous plot for \texttt{fit1}.

```{r}
plot(mom_iq,kid_score, xlab="Mother IQ score", 
  ylab="Child test score",pch=20, xaxt="n", yaxt="n", type="n", ylim = c(0, 1000))
curve (as.numeric(coef(fit1)[1]) + as.numeric(coef(fit1)[2]) + (as.numeric((coef(fit1)[3]))*x), add=TRUE, col="magenta")
curve (as.numeric(coef(fit1)[1]) + as.numeric(coef(fit1)[2]) + (as.numeric((coef(fit1)[4]))*x), col="red", add=TRUE)
curve (as.numeric(coef(fit1)[1]) + as.numeric(coef(fit1)[2]) + (as.numeric((coef(fit1)[5]))*x), add=TRUE, col="blue")
points (mom_iq[mom_work==1], kid_score[mom_work==1], pch=20,col="red")
points (mom_iq[mom_work==2], kid_score[mom_work==2], pch=20,col="magenta")
points (mom_iq[mom_work==3], kid_score[mom_work==3], pch=20,col="blue")
points (mom_iq[mom_work==4], kid_score[mom_work==4], pch=20,col="black")
axis (1, c(80,100,120,140))
axis (2, c(100,200,300,400,500,600,700,800,900,1000))
#(coef(fit1))
#equation1 <- function(x){coef(fit1)[1] + coef(fit1)[2] + coef(fit1)[3]*x + #coef(fit1)[4] + coef(fit1)[5]}
#equation2 <- function(x){coef(fit1)[1] + coef(fit1)[2] + coef(fit1)[4]*x + coef(fit1)[3] + coef(fit1)[5]}
#equation3 <- function(x){coef(fit1)[1] + coef(fit1)[2] + coef(fit1)[5]*x + coef(fit1)[3] + coef(fit1)[4]}

#ggplot(kidiq, aes(y="Child test score", x= "Mother IQ score")) + geom_point()+ stat_function(fun=equation1, geom = "line", color = scales::hue_pal()(2)[1]) + 
#stat_function(fun=equation2, geom = "line", color = scales::hue_pal()(2)[2]) + 
#stat_function(fun=equation3, geom = "line", color = scales::hue_pal()(2)[3])
```

Comment how the two models differ. Which one would you prefer and why?
The model two will be prefer. In the model 1, the prediction value is far beyond the observed value, which indicates the design model and data set does not match. In fact, both models does not show the good fit. We may need to try other models(i.e. multinomial glm model). The linear regression model does not suitable for this data set. 

When you are done, dettach \texttt{kidiq}

```{r}
detach(kidiq)
```

# Recognizing multi-collinearity in a perfect scenario

Now run the following code (again, don't forget to remove the chunk option \texttt{eval=FALSE}):

```{r}
set.seed(28)
# have the same data that are being generated artificially 
X <- cbind(1:100,2*(1:100))
# indicator 
# generating the first covariance. The first: integer from 1 to 100 (X is not full rank)
E <- rnorm(100,sd=20)
# responses
Resp <- 1+ 0.5*X[,1]+1.7*X[,2] + E
data <- data.frame(cbind(Resp,X))
# create data frame 
attach(data) 
head(data)
summary(lm(Resp~V2))
summary(lm(Resp~V2+V3))
detach(data)
```
Describe very briefly what the above code does (one or two sentences suffice).
Answer:
Generating a data frame which will get the same data every time with the first variance: integers from 1 to 100 and the second integer: two times integers from 1 to 100. (linearly dependent) And the code will compare the model \texttt{lm(Resp\textasciitilde V2)} and the model \texttt{lm(Resp\textasciitilde V2 + V3)}. 

Comment on the two models \texttt{Resp\textasciitilde V2} and \texttt\texttt{Resp\textasciitilde V2 + V3}. Which one makes sense and which one has a multi-collinearity issue? What does it mean for the corresponding design matrix?
Answer:
The model \texttt{lm(Resp\textasciitilde V2)} would have a multi-collinearity issue. (i.e. the model \texttt{lm(Resp\textasciitilde V2)} contained all the information from model \texttt{lm(Resp\textasciitilde V2 + V3)}. And the corresponding design matrix for the model \texttt{lm(Resp\textasciitilde V2)} will not be full rank.(i.e. all the columns are dependent.) The corresponding design matrix for the model \texttt{lm(Resp\textasciitilde V2 + V3)} will be full rank. 

# Recognizing multi-collinearity in an imperfect scenario (more realistic)

Now try the following code (again, don't forget to remove the chunk option \texttt{eval=FALSE}):

```{r}
set.seed(17)
X <- cbind(1:100,2*(1:100)+rnorm(100,sd=0.01))
# plus little noise
E <- rnorm(100,sd=20)
# nearly linear dependent, matrix will have the full rank
Resp <- 1+ 0.5*X[,1]+1.7*X[,2] + E
# generating reponses
data <- data.frame(cbind(Resp,X))
attach(data)
head(data)

#summary(lm(Resp~V2))
summary(lm(Resp~V2+V3))

#summary(lm(Resp~V2+V3,subset=c(1:95)))

detach(data)
```

Describe very briefly what the above code does (one or two sentences suffice). 
Answer:
Generating a data frame which will get the same data every time with the first variance: integers from 1 to 100 and the second integer: two times integers from 1 to 100.(nearly linear dependent) And the code will compare the model \texttt{lm(Resp\textasciitilde V2)} and the model \texttt{lm(Resp\textasciitilde V2 + V3)}. 

Look at the standard errors in the output of models \texttt{Resp\textasciitilde V2} and \texttt\texttt{Resp\textasciitilde V2 + V3}. Why are they so huge in the second model but not the first? 
Answer:
This shows that the model \texttt\texttt{Resp\textasciitilde V2 + V3} has the multi-collinearity issue. The design matrix is nearly singular(i.e. dividing a nearly zero.) The first model only has one predictor which could always show the good fit. 

What is the meaning of the model on line 126? Why did we consider fitting it?
Answer:
Using the first 95 lines of the data set to fit the model. If the model is healthy, the coefficients will not change a lot due to the slight decrease of the sample size. However, the line 126's model and line 125' model shows a huge different among the estimate and standard error. From the previous question, we know that the model \texttt\texttt{Resp\textasciitilde V2 + V3} has the multi-collinearity issue. In this case, the model on line 126 would be an indicator to show there is something wrong with the model.  